title: NPFL138, Lecture 5
class: title, langtech, cc-by-sa

# Convolutional Neural Networks II

## Milan Straka

### March 18, 2025

---
class: section
# Refresh

---
# Main Takeaways From Previous Lecture

- Convolutions can provide

  - local interactions in spatial/temporal dimensions
  - shift invariance
  - _much_ less parameters than a fully connected layer

~~~
- Usually repeated $3×3$ convolutions are enough, no need for larger filter
  sizes.

~~~
- When pooling is performed, double the number of channels (i.e., the first
  convolution following the pooling layer will have twice as many output
  channels).

~~~
- If your network is deep enough (the last hidden neurons have a large receptive
  fields), final fully connected layers are not needed, and global average pooling
  is enough.

~~~
- Batch normalization is a great regularization method for CNNs, allowing
  removal/decrease of dropout and $L^2$ regularization.

~~~
- Small weight decay (i.e., $L^2$ regularization) of usually 1e-4 is still useful
  for regularizing convolutional kernels.

---
section: ResNet
class: section
# ResNet

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=95%,h=center](resnet_depth_effect.svgz)

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=90%,h=center](resnet_block.svgz)

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=100%](resnet_block_reduced.svgz)

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=100%](resnet_architecture.svgz)

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=42%,mw=50%,h=center,f=left](resnet_overall.svgz)

~~~
The residual connections cannot be applied directly when
number of channels increases.

The authors considered several alternatives, and chose the one where in case of
channels increase a $1×1$ convolution + BN is used on the projections to match the
required number of channels. The required spatial resolution is achieved by
using stride 2.

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=100%,v=middle](resnet_residuals.svgz)

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=100%,v=middle](../02/nn_loss.jpg)

---
# ResNet – 2015 (3.6% ILSVRC top-5 error)

Training details:
- batch normalizations after each convolution and before activation

~~~
- SGD with batch size 256 and momentum of 0.9

~~~
- learning rate starts with 0.1 and “is divided by 10 when the error plateaus”
~~~
  - 600k training iterations are used (120 epochs, each containing 1.281M images)
  - according to one graph (and to their later paper) they decay at 25% and 50%
    of the training, so after epochs 30 and 60
~~~
    - other concurrent papers also use exponential decay or 25%-50%-75%

~~~
- no dropout, weight decay 0.0001

~~~
- during training, an image is resized with its shorter side randomly sampled
  in the range $[256, 480]$, and a random $224×224$ crop is used

~~~
- during testing, 10-crop evaluation strategy is used

~~~
  - for the best results, the scores across multiple scales are averaged – the
    images are resized so that their smaller size is in
    $\{224, 256, 384, 480, 640\}$

---
class: middle
# ResNet – 2015 (3.6% ILSVRC top-5 error)

![w=49%](resnet_validation.svgz)
![w=49%](resnet_testing.svgz)

The ResNet-34 B uses the $1×1$ convolution on residual connections with
different number of input and output channels; ResNet-34 C uses this
convolution on all residual connections. Variant B is used for
ResNet-50/101/152.
